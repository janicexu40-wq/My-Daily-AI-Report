import os
import asyncio
import feedparser
import edge_tts
from datetime import datetime
from http import HTTPStatus
import dashscope
import glob
from aligo import Aligo # ğŸ“¦ æ–°å¢ï¼šé˜¿é‡Œäº‘ç›˜å·¥å…·

# ================= 1. çŒæ‰‹é›·è¾¾è®¾ç½® =================
RSS_SOURCES = {
    "signals": [
        "https://www.v2ex.com/index.xml", 
        "https://www.reddit.com/r/SaaS/new/.rss",
    ],
    "shovels": [
        "https://news.ycombinator.com/rss",
        "https://stratechery.com/feed/",
    ],
    "macro": [
        "https://feed.36kr.com/feed",
    ]
}

# ================= 2. çŒæ‰‹æ€ç»´æ¨¡å‹ =================
HUNTER_SYSTEM_PROMPT = """
ä½ ä¸å†æ˜¯æ–°é—»æ’­æŠ¥å‘˜ï¼Œä½ æ˜¯â€œæ™¨é—´å•†ä¸šçŒæ‰‹â€ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»ä¿¡æ¯ä¸­å—…å‡ºâ€œé’±å‘³â€ã€‚
è¯·å¯¹è¾“å…¥çš„å†…å®¹è¿›è¡Œã€æ·±åº¦å•†ä¸šæ‹†è§£ã€‘ï¼Œä¸¥æ ¼éµå¾ªä»¥ä¸‹æ¡†æ¶ï¼š
... (ä¿æŒä½ ä¹‹å‰çš„ Prompt å†…å®¹ä¸å˜) ...
"""

# ================= 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================

def fetch_rss_intel(category):
    # ... (ä¿æŒåŸæœ‰çš„æŠ“å–ä»£ç ä¸å˜) ...
    print(f"ğŸ•µï¸â€â™‚ï¸ [çŒæ‰‹é›·è¾¾] æ­£åœ¨æ‰«æ {category} é¢‘é“...")
    combined_content = ""
    for url in RSS_SOURCES.get(category, []):
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries[:3]:
                title = getattr(entry, 'title', 'æ— æ ‡é¢˜')
                link = getattr(entry, 'link', 'æ— é“¾æ¥')
                summary = getattr(entry, 'summary', '')[:200] 
                combined_content += f"ã€æ ‡é¢˜ã€‘{title}\nã€é“¾æ¥ã€‘{link}\nã€æ‘˜è¦ã€‘{summary}\n\n"
        except Exception as e:
            print(f"âš ï¸ æŠ“å–å¤±è´¥ {url}: {e}")
    return combined_content

def analyze_with_hunter_ai(content):
    # ... (ä¿æŒåŸæœ‰çš„ AI åˆ†æä»£ç ä¸å˜) ...
    if not content: return "æ— æœ‰æ•ˆä¿¡å·ã€‚"
    try:
        dashscope.api_key = os.getenv("DASHSCOPE_API_KEY")
        response = dashscope.Generation.call(
            model=dashscope.Generation.Models.qwen_turbo, 
            messages=[{'role': 'system', 'content': HUNTER_SYSTEM_PROMPT},
                      {'role': 'user', 'content': content}]
        )
        if response.status_code == HTTPStatus.OK: return response.output.text
        else: return "AI æš‚æ—¶æ‰çº¿ã€‚"
    except Exception as e: return f"é”™è¯¯: {e}"

# ğŸ”¥ æ–°å¢åŠŸèƒ½ï¼šä¸Šä¼ æ‰€æœ‰æ–‡ä»¶åˆ°é˜¿é‡Œäº‘ç›˜
def upload_to_aliyun_drive(file_paths):
    print("â˜ï¸ [äº‘ç«¯å½’æ¡£] æ­£åœ¨è¿æ¥é˜¿é‡Œäº‘ç›˜...")
    try:
        # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ Refresh Token ç™»å½•
        refresh_token = os.getenv("ALIYUN_REFRESH_TOKEN")
        if not refresh_token:
            print("âŒ æœªæ‰¾åˆ° ALIYUN_REFRESH_TOKENï¼Œè·³è¿‡ä¸Šä¼ ã€‚")
            return

        ali = Aligo(level=logging.INFO, refresh_token=refresh_token)
        
        # ç›®æ ‡æ–‡ä»¶å¤¹ (å¦‚æœä¸å­˜åœ¨ä¼šè‡ªåŠ¨åˆ›å»º)
        remote_folder = ali.get_folder_by_path('/æ™¨é—´æƒ…æŠ¥')
        if not remote_folder:
            ali.create_folder('/æ™¨é—´æƒ…æŠ¥')
            remote_folder = ali.get_folder_by_path('/æ™¨é—´æƒ…æŠ¥')

        # æ‰¹é‡ä¸Šä¼ 
        for file_path in file_paths:
            if os.path.exists(file_path):
                print(f"   â¬†ï¸ æ­£åœ¨ä¸Šä¼ : {os.path.basename(file_path)}")
                ali.upload_file(file_path, remote_folder.file_id)
        
        print("âœ… æ‰€æœ‰æ–‡ä»¶å·²å¤‡ä»½è‡³é˜¿é‡Œäº‘ç›˜ï¼")
    except Exception as e:
        print(f"âŒ ä¸Šä¼ å¤±è´¥ (ä¸å½±å“æœ¬åœ°ç”Ÿæˆ): {e}")

# ğŸ§¹ æ‰«åœ°åƒ§ï¼šåªä¿ç•™æœ€è¿‘ 3 å¤©
def cleanup_old_files(output_dir="output", days_to_keep=3):
    print("ğŸ§¹ [æ‰«åœ°åƒ§] å¼€å§‹æ¸…ç† GitHub æœ¬åœ°æ—§æ–‡ä»¶...")
    now = time.time()
    cutoff = now - (days_to_keep * 86400)
    
    files = glob.glob(os.path.join(output_dir, "*"))
    for f in files:
        if os.path.basename(f).startswith("."): continue # è·³è¿‡éšè—æ–‡ä»¶
        
        # å¦‚æœæ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´æ—©äºæˆªæ­¢æ—¶é—´ï¼Œåˆ™åˆ é™¤
        if os.stat(f).st_mtime < cutoff:
            try:
                os.remove(f)
                print(f"   ğŸ—‘ï¸ å·²ä»ä»“åº“ç§»é™¤è¿‡æœŸæ–‡ä»¶: {os.path.basename(f)}")
            except Exception as e:
                print(f"   âŒ åˆ é™¤å¤±è´¥: {e}")

# ================= 4. ä¸»ç¨‹åºå…¥å£ =================

async def main():
    today_str = datetime.now().strftime("%Y%m%d")
    output_dir = "output"
    if not os.path.exists(output_dir): os.makedirs(output_dir)

    # 1. æŠ“å–ä¸åˆ†æ
    print("ğŸš€ æ™¨é—´çŒæ‰‹ä»»åŠ¡å¯åŠ¨...")
    signals = fetch_rss_intel("signals")
    shovels = fetch_rss_intel("shovels")
    macro = fetch_rss_intel("macro")
    full_intel = f"=== ç„¦è™‘ä¿¡å· ===\n{signals}\n=== æ˜é‡‘é“²å­ ===\n{shovels}\n=== å®è§‚é£å‘ ===\n{macro}"
    
    report = analyze_with_hunter_ai(full_intel)
    
    # 2. ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶ (MD, MP3, HTML)
    files_to_upload = []

    # MD
    md_path = os.path.join(output_dir, f"briefing_{today_str}.md")
    with open(md_path, "w", encoding="utf-8") as f: f.write(report)
    files_to_upload.append(md_path)

    # MP3
    mp3_path = os.path.join(output_dir, f"briefing_{today_str}.mp3")
    tts_text = report[:1000] # æˆªå–å‰1000å­—æ’­æŠ¥
    communicate = edge_tts.Communicate(tts_text, "zh-CN-YunxiNeural")
    await communicate.save(mp3_path)
    files_to_upload.append(mp3_path)

    # HTML
    html_path = os.path.join(output_dir, f"briefing_{today_str}.html")
    html_content = f"""<html><body><h1>{today_str}</h1><audio controls src="briefing_{today_str}.mp3"></audio><pre>{report}</pre></body></html>"""
    with open(html_path, "w", encoding="utf-8") as f: f.write(html_content)
    files_to_upload.append(html_path)

    # 3. â˜ï¸ å…ˆå¤‡ä»½ï¼šä¸Šä¼ æ‰€æœ‰æ–‡ä»¶åˆ°é˜¿é‡Œäº‘ç›˜ (å…³é”®æ­¥éª¤ï¼)
    # è¿™ä¸€æ­¥ç¡®ä¿äº†æ— è®º GitHub æ€ä¹ˆåˆ ï¼Œäº‘ç›˜é‡Œæ°¸è¿œæœ‰ä¸€ä»½å…¨é‡çš„
    upload_to_aliyun_drive(files_to_upload)

    # 4. ğŸ§¹ åæ¸…ç†ï¼šåˆ é™¤ GitHub 3å¤©å‰çš„æ—§æ–‡ä»¶
    cleanup_old_files(output_dir, days_to_keep=3)

if __name__ == "__main__":
    import logging
    asyncio.run(main())
